<!DOCTYPE article PUBLIC "-//OASIS//DTD Simplified DocBook XML V4.1.2.5//EN"
        ".dbx/lib/docbook/simple-dtd/sdocbook.dtd">
<article>

<!-- Header -->

<articleinfo>

  <!-- title of HOWTO, include the word HOWTO -->

<title>HOWTO-For Setting Up Biopipe</title>
<author>
  <firstname>Shawn</firstname>
  <surname>Hoon</surname>
  <affiliation>
    <address>
      <email>shawnh@fugu-sg.org</email>
    </address>
  </affiliation>
</author>

<revhistory>
  <revision>
    <revnumber>0.2</revnumber>
    <date>2003-02-19</date>
    <authorinitials>savikalpa</authorinitials>
    <revremark>Second Draft(BioHackathon 2003)</revremark>
  </revision>
</revhistory>

<!--<legalnotice>
<para>
This document is copyright Shawn Hoon, 2002. For reproduction other than
personal use, please contact me at shawnh@fugu-sg.org
</para>
</legalnotice>//-->

<abstract>
<para>
This is a HOWTO written in DocBook (SGML) for the installation of the biopipe,
its workings and definition.We will describe how to load and run a simple blast
pipeline and a protein analysis pipeline.
</para>
</abstract>

 </articleinfo>


<!-- Section1: intro -->

<section id="intro">
  <title>Introduction</title>

  <indexterm>
    <primary>disk!introduction</primary>
  </indexterm>

  <para>
  The bioperl pipeline framework is a flexible workflow system that complements the bioperl
  package in providing job managment facilities for high throughput analysis. 
  This system is heavily inspired by the EnsEMBL Pipeline system. 
  This section describes the design of the bioperl pipeline. 
  Some features of the current system include:
  </para>

  <itemizedlist>
    <listitem>
    <para>
      Handling of various input and output data formats from various databases.
    </para>
  </listitem>
  <listitem>
    <para>
    A bioperl interface to non-specific loadsharing software (LSF,PBS etc) to ensure that
    the various analysis programs are run in proper order and are successfully completed 
    while re-running those that fail.
    </para>
  </listitem>
  <listitem>
    <para>
     A flexible pluggable bioperl interface that allows programs to be 'pipeline-enabled'.
    </para>
  </listitem>
  </itemizedlist>

 <para>
 Setting up bioinformatics pipeline is not trivial. This tutorial introduces some aspects of 
 biopipe through setting up a very simple blast pipeline and a protein analysis pipeline.It is hope that through this tutorial,
 two objectives are achieved:
 <itemizedlist>
 <listitem>
 <para>
  Iron out installation issues using a simple example. 
  </para>
  </listitem>
  <listitem>
  <para>
  Familiarization with the biopipe system and introduction of the XML system we have
  develop to ameliorate some of the complexities involve in setting up.
  </para>
  </listitem>
  </itemizedlist>
 </para>
 </section>
 
 <section id="installation">
  <title>Installation</title>
  <para>The following sections descibes how to install your pipeline from start to finish</para>
  </section>


 <section id="configuration">
  <title>The Pipeline XML Format</title>
  <para>This section describes the various sections of XML with Biopipe</para>

  <para>
  To describe this, we will use the demo xml template, blast_file_pipeline.xml. 
  You may find this in the bioperl-pipeline/xml/templates directory.
  </para>

  <para>
  <emphasis>XML Organization</emphasis>
  </para>
<screen>
  &lt;pipeline_setup&gt;
    &lt;database_setup&gt;
    &lt;iohandler_setup&gt;
    &lt;pipeline_flow_setup&gt;
    &lt;job_setup&gt;(optional)
  &lt;/pipeline_setup&gt;
</screen>

  <para> 
  &lt;database_setup&gt;
  </para>
  <para>
  This specifies the databases that the pipeline connects to and the
  adaptor modules that intefaces with them.
  </para>
  <para>
  &lt;iohandler_setup&gt;
  </para>
  <para>
  This specifies the method calls that will be used by the pipeline
  to access the databases. These methods are contained in the modules
  specified by the database setup section above.
  </para>

  <para>
  &lt;pipeline_flow_setup&gt;
  </para>
  <para>
  This specifies the analysis and rules of the pipeline. Analysis
  refers to the runnables that will be used in this pipeline while the
  rules specify the order in which these analysis are to be run, including
  any specific pre-processing actions that are to be carried out.
  </para>

  <para>
  &lt;job_setup&gt;
  </para>
  <para>
  This is an optional part that allows specific inputs to be inserted.
  Usually, this is done using DataMongers and Input Create modules.
  </para>

  <para>You will need to modify some parts of this XML file to point files to non-default places.</para>

  </section>

  <section id="blast">
    <title>The Simple Blast Pipeline </title>
    <para> <emphasis>Use Case</emphasis> </para>
    <screen>
    Given a file of sequences, split the files into smaller chunks, and blast
    it against the database over a compute farm. Blast results files are stored
    into a given results  directory, with one result file per blast job.
    </screen>
    <para>
    This is a simple blast pipeline demo that allows one to pipeline a bunch of blast
    jobs. It is stripped bare, assuming that the user has sequences in files and simply wishes
    to organize and manage the blast jobs(executed serially in a single node/local environment). 
    Additionaly the pipeline parallalizes the blast jobs if multiple compute nodes are available.
    It doesn't utilize one of the main features of blast which is to allow inputs from different 
    database sources.
    </para>

    <para><emphasis>Configuring the Pipeline</emphasis></para>
    
    <para> ANALYSIS 1: DataMonger </para>
    <para>
    This involves a DataMonger Analysis using the <emphasis>setup_file_blast</emphasis> module.
        The datamonger will split the input file specified into a specified number of chunks.
        It will create a blast job in the pipeline for each chunk. It will also
        create the specified working directory for storing these files and format the db file for
        blasting if you are blasting against itself. If you are blasting against a different database file,
        you can specify the formatting of the db as part of the analysis parameters. 
    </para>
    <para>
    <programlisting>
    262     &lt;analysis id="1"&gt;
    263       &lt;data_monger&gt;
    264         &lt;initial/&gt;
    265         &lt;input&gt;
    266           &lt;name&gt;input_file&lt;/name&gt;
    267         &lt;/input&gt;
    268         &lt;input_create&gt;
    269            &lt;module&gt;setup_file_blast&lt;/module&gt;
    270            &lt;rank&gt;1&lt;/rank&gt;
    271             &lt;argument&gt;
    272                 &lt;tag&gt;input_file&lt;/tag&gt;
    273                 &lt;value&gt;../t/data/blast.fa&lt;/value&gt;
    274                 &lt;type&gt;SCALAR&lt;/type&gt;
    275             &lt;/argument&gt;
    276             &lt;argument&gt;
    277                 &lt;tag&gt;full_path&lt;/tag&gt;
    278                 &lt;value&gt;1&lt;/value&gt;
    279                 &lt;type&gt;SCALAR&lt;/type&gt;
    280             &lt;/argument&gt;
    281             &lt;argument&gt;
    282                 &lt;tag&gt;chop_nbr&lt;/tag&gt;
    283                 &lt;value&gt;5&lt;/value&gt;
    284                 &lt;type&gt;SCALAR&lt;/type&gt;
    285             &lt;/argument&gt;
    286             &lt;argument&gt;
    287                 &lt;tag&gt;workdir&lt;/tag&gt;
    288                 &lt;value&gt;/tmp/blast_dir/&lt;/value&gt;
    289                 &lt;type&gt;SCALAR&lt;/type&gt;
    290             &lt;/argument&gt;
    291             &lt;argument&gt;
    292                 &lt;tag&gt;result_dir&lt;/tag&gt;
    293                 &lt;value&gt;/tmp/blast_dir/blast_result/&lt;/value&gt;
    294                 &lt;type&gt;SCALAR&lt;/type&gt;
    295             &lt;/argument&gt;
    296             &lt;argument&gt;
    297                 &lt;tag&gt;runnable&lt;/tag&gt;
    298                 &lt;value&gt;Bio::Pipeline::Runnable::Blast&lt;/value&gt;
    299                 &lt;type&gt;SCALAR&lt;/type&gt;
    300             &lt;/argument&gt;
    301             &lt;argument&gt;
    302                 &lt;tag&gt;informat&lt;/tag&gt;
    303                 &lt;value&gt;fasta&lt;/value&gt;
    304                 &lt;type&gt;SCALAR&lt;/type&gt;
    305             &lt;/argument&gt;
    306             &lt;argument&gt;
    307                 &lt;tag&gt;outformat&lt;/tag&gt;
    308                 &lt;value&gt;fasta&lt;/value&gt;
    309                 &lt;type&gt;SCALAR&lt;/type&gt;
    310             &lt;/argument&gt;
    311             &lt;argument&gt;
    312                 &lt;tag&gt;tag&lt;/tag&gt;
    313                 &lt;value&gt;infile&lt;/value&gt;
    314                 &lt;type&gt;SCALAR&lt;/type&gt;
    315             &lt;/argument&gt;
    316          &lt;/input_create&gt;
    317       &lt;/data_monger&gt;
    318     &lt;/analysis&gt;
    
    </programlisting>
    </para>
    <para>
    line 269: This specifies the particular DataMonger to use that will prepare your file for
    paralization. For this case, we will use setup_file_blast which will chop up your
    input file specified below into smaller chunks. 
    </para>
    <para>
    line 273: This specifies the name of the input file that will be split into smaller chunks.
    Modify this accordingly.
    </para>
    <para>
    line 283: This specifies the number of files to split the input file into which wil equal
              the number of blast jobs. You will want to chose a reasonable number that will
              utilize your compute farm best.
    </para>
    <para>
    line 288: This specifies the working directory in which the blast chunks will be stored.
    </para>
    <para>
    line: 293: This specifies where the blast result files will be stored.
    </para> 
   </section>
   <section id="configureblast2">
    <title>The Simple Blast Pipeline</title>
    <para> ANALYSIS 2: Blast</para>
   
    <programlisting>
    321     &lt;analysis id="2"&gt;
    322       &lt;logic_name&gt;Blast&lt;/logic_name&gt;
    323       &lt;runnable&gt;Bio::Pipeline::Runnable::Blast&lt;/runnable&gt;
    324       &lt;db&gt;family&lt;/db&gt;
    325       &lt;db_file&gt;..t/data/blast.fa&lt;/db_file&gt;
    326       &lt;program&gt;blastall&lt;/program&gt;
    327       &lt;program_file&gt;&lt;/program_file&gt;
    328       &lt;analysis_parameters&gt;-p blastp -e 1-e05 &lt;/analysis_parameters&gt;
    329       &lt;runnable_parameters&gt;-formatdb 1 -result_dir /tmp/blast_dir/blast_result/&lt;/runnable_parameters&gt;
    330     &lt;/analysis&gt;
    </programlisting>

   <para>
   This set of xml tags specify the blast analysis to run.
   </para>
   <para>
   Line 323: This specifies the pipeline to use the <emphasis>Bio::Pipeline::Runnable::Blast</emphasis> runnable.
   </para>
   <para>
   Line 324: This is the name of the database to blast against
   </para>
   <para>
   Line 325: This is the path to the database file to blast against.
   </para>
   <para>
   Line 326: This is the name of the blast program.
   </para>
   <para>
   Line 327: This is the location of the blast program
   </para>
   <para>
   Line 328: These are the blast parameters.
   </para>
   <para>
   Line 329: These are blast runnable parameters.
   </para>
  <itemizedlist>
    <listitem>
    <para>
      <emphasis>-p blastp</emphasis> This is a blastall parameter to specify using the  blastp alignment program.
    </para>
  </listitem>
  <listitem>
    <para>
    <emphasis>-e 1-e05</emphasis> Another blastall parameter to return hits with scores less than 0.00001
    </para>
  </listitem>
  <listitem>
    <para>
    <emphasis>-formatdb 1</emphasis> A Bio::Pipeline::Runnable::Blast parameter that tells it to format the db file specified in line 307 before
    commencing blasting.
    </para>
  </listitem>
  <listitem>
    <para>
    <emphasis>-result_dir /tmp/blast_dir/blast_reult/</emphasis> This tells the Runnable where to store the blast output. 
    </para>
  </listitem>

  </itemizedlist>
  </section>
  
  <section id="protein">
  <title> The Protein Analysis Pipeline</title>
  <screen>
    Given a file of protein sequences,obtain the seq objects using Bio::DB::Fasta and run 6 different protein analysis on them.
    Results are dumped into a file in the gff format.
  </screen>
   <para><emphasis>Configuring the Pipeline</emphasis></para>

   <para> ANALYSIS 1: DataMonger </para>
   <para>
    This involves a DataMonger Analysis using the <emphasis>setup_file_blast</emphasis> module.
    The datamonger will split the input file specified into a specified number of chunks.
    It will create a blast job in the pipeline for each chunk. It will also
    create the specified working directory for storing these files and format the db file for
    blasting if you are blasting against itself. If you are blasting against a different database file,
    you can specify the formatting of the db as part of the analysis parameters.
   </para>
   <para>
   <programlisting>
111    <database_setup>
112     <streamadaptor id="1">
113       <module>Bio::Pipeline::Dumper</module>
114     </streamadaptor>
115     <streamadaptor id="2">
116       <module>Bio::DB::Fasta</module>
117     </streamadaptor>
118    </database_setup>
   </programlisting> 
  
   <programlisting>
119    <iohandler_setup>
120     <iohandler id="2">
121      <adaptor_id>2</adaptor_id>
122      <adaptor_type>STREAM</adaptor_type>
123      <iohandler_type>INPUT</iohandler_type>
124     <method>
125        <name>new</name>
126        <rank>1</rank>
127        <argument>
128            <value>../t/data/prot_input2.fa</value>
129        </argument>
130     </method>
131     <method>
132        <name>get_Seq_by_id</name>
133        <argument>
134           <value>INPUT</value>
135        </argument>
136        <rank>2</rank>
137     </method>
138    </iohandler>
    
141     <iohandler id="1">
142      <adaptor_id>2</adaptor_id>
143      <adaptor_type>STREAM</adaptor_type>
144      <iohandler_type>INPUT</iohandler_type>
145      <method>
146        <name>new</name>
147        <rank>1</rank>
148        <argument>
149          <value>../t/data/prot_input2.fa</value>
150        </argument>
151      </method>
152      <method>
153        <name>get_all_ids</name>
154        <rank>2</rank>
155      </method>
156    </iohandler>

158    <iohandler id="3">
159      <adaptor_id>1</adaptor_id>
160      <adaptor_type>STREAM</adaptor_type>
161      <iohandler_type>OUTPUT</iohandler_type>
162      <method>
163        <name>new</name>
164        <rank>1</rank>
165        <argument>
166          <tag>-dir</tag>
167          <value>../t/data/protein_analysis_results</value>
168          <type>SCALAR</type>
169          <rank>1</rank>
170        </argument>
171        <argument>
172          <tag>-module</tag>
173          <value>generic</value>
174          <type>SCALAR</type>
175          <rank>1</rank>
176        </argument>
177        <argument>
178          <tag>-prefix</tag>
179          <type>SCALAR</type>
180          <value>INPUT</value>
181          <rank>2</rank>
182        </argument>
183        <argument>
184          <tag>-format</tag>
185          <type>SCALAR</type>
186          <value>gff</value>
187          <rank>3</rank>
188        </argument>
189        <argument>
190          <tag>-file_suffix</tag>
191          <type>SCALAR</type>
192          <value>gff</value>
193          <rank>4</rank>
194        </argument>
195      </method>
196      <method>
197        <name>dump</name>
198        <rank>2</rank>
199        <argument>
200          <value>OUTPUT</value>
201          <type>ARRAY</type>
202          <rank>1</rank>
203        </argument>
204       </method>
205      </iohandler>
206   </iohandler_setup>
   </programlisting>  
   
   <programlisting>
209     <analysis id="1">
210      <data_monger>
211        <initial/>
212        <input>
213          <name>protein_ids</name>
214          <iohandler>1</iohandler>
215        </input>
216        <input_create>
217           <module>setup_initial</module>
218           <rank>1</rank>
219           <argument>
220                <tag>protein_ids</tag>
221                <value>2</value>
222            </argument>
223         </input_create>
224      </data_monger>
225      <input_iohandler id="1"/>
226    </analysis>

229    <analysis id="2">
230      <logic_name>Tmhmm</logic_name>
231      <runnable>Bio::Pipeline::Runnable::ProteinAnnotation</runnable>
232      <program>Tmhmm</program>
233      <program_file>/usr/users/pipeline/programs/TMHMM2.0b/bin/tmhmm</program_file>
234      <runnable_parameters>-program Tmhmm</runnable_parameters>
235      <!-- Specify which iohandler to use fetch the sequence -->
236      <input_iohandler id="2"/>
237      <!-- Specify which iohandler to use to store the protein features --> 
238      <output_iohandler id="3"/>
239    </analysis>
    
    <!--Analysis 2 Seg --> 
242   <analysis id="3">
243      <logic_name>Seg</logic_name>
244      <runnable>Bio::Pipeline::Runnable::ProteinAnnotation</runnable>
245      <program>Seg</program>
246      <program_file>/usr/users/pipeline/programs/seg_dir/seg</program_file>
247      <runnable_parameters>-program Seg</runnable_parameters>
248      <input_iohandler id="2"/>
249      <output_iohandler id="3"/>
250    </analysis>

    !--Analysis 3 Prints -->
253    <analysis id="4">
254      <logic_name>Prints</logic_name>
255      <runnable>Bio::Pipeline::Runnable::ProteinAnnotation</runnable>
256      <db>prints.dat</db>
257      <db_file>../t/data/prints.dat</db_file>
258      <program>Prints</program>
259      <program_file>/usr/users/pipeline/programs/Prints</program_file>
260     <runnable_parameters>-program Prints</runnable_parameters>
261      <input_iohandler id="2"/>
262      <output_iohandler id="3"/>
263   </analysis>
    
    <!--Analysis 4 Profile -->
266    <analysis id="5">
267      <logic_name>Profile</logic_name>
268      <runnable>Bio::Pipeline::Runnable::ProteinAnnotation</runnable>
269      <db>prosite.dat</db>
270      <db_file>../t/data/prosite.dat</db_file>
271      <program>Profile</program>
272      <program_file>/usr/users/pipeline/programs/pfscan</program_file>
273      <runnable_parameters>-program Profile</runnable_parameters>
274      <input_iohandler id="2"/>
275      <output_iohandler id="3"/>
276    </analysis>

    <!--Analysis 5 Signalp --> 
279    <analysis id="6">
280      <logic_name>Signalp</logic_name>
281      <runnable>Bio::Pipeline::Runnable::ProteinAnnotation</runnable>
282      <program>SignalP</program>
283      <program_file>/usr/users/pipeline/programs/signalp_2.0/signalp</program_file>
284      <runnable_parameters>-program Signalp</runnable_parameters>
285     <input_iohandler id="2"/>
286      <output_iohandler id="3"/>
287    </analysis>
    
    <!--Analysis 6 Hmmpfam -->
290    <analysis id="7">
291      <logic_name>Hmmpfam</logic_name>
292      <runnable>Bio::Pipeline::Runnable::ProteinAnnotation</runnable>
293      <db>Pfam_ls_7.4</db>
294      <db_file>../t/data/pfam_sample_R11</db_file>
295      <program>hmmpfam</program>
296      <program_file>/usr/local/bin/hmmpfam</program_file>
297      <runnable_parameters>-program Hmmpfam</runnable_parameters>
298      <input_iohandler id="2"/>
299      <output_iohandler id="3"/>
300    </analysis>

   </programlisting>
   
   <programlisting>
306    <rule>
307      <current_analysis_id>1</current_analysis_id>
308     <next_analysis_id>2</next_analysis_id>
309      <action>NOTHING</action>
310    </rule>
311    <rule>
312      <current_analysis_id>2</current_analysis_id>
313     <next_analysis_id>3</next_analysis_id>
314      <action>COPY_ID</action>
315    </rule>
316    <rule>
317      <current_analysis_id>3</current_analysis_id>
318     <next_analysis_id>4</next_analysis_id>
319      <action>COPY_ID</action>
320    </rule>
321    <rule>
322      <current_analysis_id>4</current_analysis_id>
323     <next_analysis_id>5</next_analysis_id>
324      <action>COPY_ID</action>
325    </rule>
326    <rule>
327      <current_analysis_id>5</current_analysis_id>
328     <next_analysis_id>6</next_analysis_id>
329      <action>COPY_ID</action>
330    </rule>
331    <rule>
332      <current_analysis_id>6</current_analysis_id>
333     <next_analysis_id>7</next_analysis_id>
334      <action>COPY_ID</action>
335    </rule>

   </programlisting>

  </para>
  </section> 
  <section id="configure2">
  <title> Configuring the PipeConf</title>

  <para>
  Hangon. You may want to configure any pipeline management parameters before running. This is done via
  the PipeConf.pm module located at <emphasis>bioperl-pipeline/Bio/Pipeline/PipeConf.pm</emphasis>.
  Various parameters may be set here:
  </para>
  <programlisting>must be changed
     39 %PipeConf = (
     40
     41     # You will need to modify these variables
     42
     43     # working directory for err/outfiles
     44     NFSTMP_DIR => '/tmp/',
     45
     46     # database specific variables
     47
     48     DBI_DRIVER => 'mysql',
     49     DBHOST     => 'mysql',
     50     DBNAME     => 'test_xml',
     51     DBUSER     => 'root',
     52     DBPASS     => '',
     53
     54     # Batch Management system module
     55     # Currently supports PBS and LSF
     56     # ignored if run in local mode
     57     BATCH_MOD   =>  'LSF',
     58
     59     # farm queue
     60     QUEUE      => 'normal3',
     61
     62     # no of jobs to send to Batch Management system at one go
     63     BATCHSIZE  => 3,
     64
     65     #bsub opt
     66     BATCH_PARAM => '-C0',
     67
     68     # no of jobs to fetch at a time and submit
     69     MAX_INCOMPLETE_JOBS_BATCHSIZE => 1000,
     70
     71     # no of completed jobs to fetch at a time and create next jobs
     72     MAX_CREATE_NEXT_JOBS_BATCHSIZE => 5,
     73
     74
     75     # number of times to retry a failed job
     76     RETRY       => '5',
     77
     78     # path to runner.pl, use by the BatchSubmission objects
     79     # to look for runner.pl. If not supplied it looks in the default
     80     # directory where PipelineManager lies
     81     RUNNER     => '',
     82
     83     #sleep time (seconds) in PipelineManager before waking up and looking for jobs to run
     84     SLEEP      => 30,
     85     #number of jobs to fetch at a time
     86     FETCH_JOB_SIZE => 100,

    </programlisting>
</section>  

  <section id="Loading and Running">
    <title>Loading and Running your pipeline </title>
    <para>
    To load and run the pipeline, cd to the <emphasis>bioperl-pipeline/scripts/</emphasis> directory.
    </para>
    <para> 
    Run the PipelineManger with the -h option to check the options available: 
    </para>
    <programlisting>
shawnh@pulse1 ~/src/bioperl-pipeline/scripts> perl PipelineManager -h
************************************
*PipelineManager
************************************
This is the central script used to run the pipeline.

Usage: PipelineManager -dbname test_pipe -xml template/blast_file_pipeline.xml -local

Options:
Default values are read from PipeConf.pm

     -dbhost    The database host name (localhost)
     -dbname    The pipeline database name
     -dbuser    User for connecting to db (root)
     -dbpass    The password to mysql database
     -dbdriver  Database driver (mysql)
     -schema    The Biopipe database schema (../sql/schema)
     -xml       The xml pipeline template file. It will run XMLImporter if provided
     -xf        Force drop of any existing Biopipe database with the same name
     -flush     flush all locks on pipeline and remove any that exists.
                Should only be used for debugging or development.
     -batchsize The number ofjobs to be batched to one node
     -local     Whether to run jobs in local mode
                (on the node where this script is run)
     -jobnbr    Number of jobs to run (for testing)
     -queue     Specify the queue on which to submit jobs
     -retry     Number of times to retry failed jobs
     -notest    Don't run pre-pipeline checks
     -norun     Use when you just want to load the XML without running
     -verbose   Whether to show warning during test and setup
     -help      Display this help

    </programlisting>

    <para>
    If you run the script in local(-l) mode(shown below for the blast pipeline), the script will not be batched to LSF or PBS. Jobs are executed
    sequentially in this way This is usually a recommended way of testing your pipeline before batching all the jobs. 
    You may also specify the number of jobs to run with the -jobnbr option.
    </para>
    <programlisting>
    savikalpa@pulse1 ~/src/bioperl-pipeline/scripts> perl PipelineManager -dbname blast_pipe -xml ../xml/templates/blast_file_pipeline.xml -loca    l -flush
    
    A database called blast_pipe already exists.
    Continuing would involve dropping this database and loading a fresh one using ../xml/templates/blast_file_pipeline.xml.
    Would you like to continue? y/n [n] y
    Dropping Databases
    Creating blast_pipe
    Loading Schema...
    Reading Data_setup xml   : ../xml/templates/blast_file_pipeline.xml
    Doing DBAdaptor and IOHandler setup
    Doing Pipeline Flow Setup
    Doing Analysis..
    Doing Rules
    Doing Job Setup...
    Loading of pipeline blast_pipe completed
    Removing Lock File...
    Done
    ///////////////////////////////////////////////////////////
    2 analysis found.
    Running test and setup..

    //////////// Analysis Test ////////////
    Checking Analysis 1 DataMonger
    -------------------- WARNING ---------------------
    MSG: Skipping test for DataMonger
    ---------------------------------------------------
    ok
    Checking Analysis 2 Blast ok
    Fetching Jobs...
    Fetched 1 incomplete jobs
    Running job /tmp/blast_pipe_DataMonger.1045639141.463.out /tmp/blast_pipe_DataMonger.1045639141.463.err
    Fetched 1 completed jobs
    Going to snooze for 3 seconds...
    Waking up and run again!
    Fetching Jobs...
    Fetched 5 incomplete jobs
    Running job /tmp/blast_pipe_Blast.1045639141.786.out /tmp/blast_pipe_Blast.1045639141.786.err
    Running job /tmp/blast_pipe_Blast.1045639141.58.out /tmp/blast_pipe_Blast.1045639141.58.err
    Running job /tmp/blast_pipe_Blast.1045639141.346.out /tmp/blast_pipe_Blast.1045639141.346.err
    Running job /tmp/blast_pipe_Blast.1045639141.817.out /tmp/blast_pipe_Blast.1045639141.817.err
    Running job /tmp/blast_pipe_Blast.1045639141.710.out /tmp/blast_pipe_Blast.1045639141.710.err
    Fetched 5 completed jobs
    Updating Completed Jobs and creating new ones
    Going to snooze for 3 seconds...
    Waking up and run again!
    Nothing left to run.

    ///////////////Shutting Down Pipeline//////////////////////
    Removing Lock File...
    Done
    ///////////////////////////////////////////////////////////

</programlisting>
  <para>
  The following example below shows the output one might get when running the protein analysis pipeline. Two sequence are used 
  for the following example and 6 protein analysis are performed on them. This also gives an idea of the possible output when using the pipeline  to run multiple analysis.
  </para>
   <programlisting>
    savikalpa@pulse1 ~/src/bioperl-pipeline/scripts> perl  PipelineManager -dbname protein_analysis_pipe -xml ../xml/templates/protein_file_pipe    line.xml -l -f
    A database called protein_analysis_pipe already exists.
    Continuing would involve dropping this database and loading a fresh one using ../xml/templates/protein_file_pipeline.xml.
    Would you like to continue? y/n [n] y
    Dropping Databases
    Creating protein_analysis_pipe
    Loading Schema...
    Reading Data_setup xml   : ../xml/templates/protein_file_pipeline.xml
    Doing DBAdaptor and IOHandler setup
    Doing Converters..
    Doing Pipeline Flow Setup
    Doing Analysis..
    Doing Rules
    Doing Job Setup...
    Loading of pipeline protein_analysis_pipe completed
    Removing Lock File...
    Done
    ///////////////////////////////////////////////////////////
    7 analysis found.
    Running test and setup..

    //////////// Analysis Test ////////////
    Checking Analysis 1 DataMonger
    -------------------- WARNING ---------------------
    MSG: Skipping test for DataMonger
    ---------------------------------------------------
    ok
    Checking Analysis 2 Tmhmm ok
    Checking Analysis 3 Seg ok
    Checking Analysis 4 Prints ok
    Checking Analysis 5 Profile ok
    Checking Analysis 6 Signalp ok
    Checking Analysis 7 Hmmpfam ok
    Fetching Jobs...
    Fetched 1 incomplete jobs
    Running job /tmp/protein_analysis_pipe_DataMonger.1045801650.61.out /tmp/protein_analysis_pipe_DataMonger.1045801650.61.err
    Fetched 1 completed jobs
    Going to snooze for 3 seconds...
    Waking up and run again!
    Fetching Jobs...
    Fetched 2 incomplete jobs
    Running job /tmp/protein_analysis_pipe_Tmhmm.1045801651.1.out /tmp/protein_analysis_pipe_Tmhmm.1045801651.1.err
    Running job /tmp/protein_analysis_pipe_Tmhmm.1045801651.807.out /tmp/protein_analysis_pipe_Tmhmm.1045801651.807.err
    Fetched 2 completed jobs
    Updating Completed Jobs and creating new ones
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_4.Seg.1045801652.736.out /tmp/protein_analysis_pipe_.job_4.Seg.1045801652.736.err
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_5.Seg.1045801652.252.out /tmp/protein_analysis_pipe_.job_5.Seg.1045801652.252.err
    Going to snooze for 3 seconds...
    Waking up and run again!
    Fetching Jobs...
    Fetched 0 incomplete jobs
    Fetched 2 completed jobs
    Updating Completed Jobs and creating new ones
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_6.Prints.1045801652.27.out /tmp/protein_analysis_pipe_.job_6.Prints.1045801652.27.err
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_7.Prints.1045801652.365.out /tmp/protein_analysis_pipe_.job_7.Prints.1045801652.365.err
    Going to snooze for 3 seconds...
    Waking up and run again!
    Fetching Jobs...
    Fetched 0 incomplete jobs
    Fetched 2 completed jobs
    Updating Completed Jobs and creating new ones
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_8.Profile.1045801655.151.out /tmp/protein_analysis_pipe_.job_8.Profile.1045801655.151.err
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_9.Profile.1045801655.710.out /tmp/protein_analysis_pipe_.job_9.Profile.1045801655.710.err
    Going to snooze for 3 seconds...
    Waking up and run again!
    Fetching Jobs...
    Fetched 0 incomplete jobs
    Fetched 2 completed jobs
    Updating Completed Jobs and creating new ones
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_10.Signalp.1045801658.115.out /tmp/protein_analysis_pipe_.job_10.Signalp.1045801658.115.err
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_11.Signalp.1045801661.890.out /tmp/protein_analysis_pipe_.job_11.Signalp.1045801661.890.err
    Going to snooze for 3 seconds...
    Waking up and run again!
    Fetching Jobs...
    Fetched 0 incomplete jobs
    Fetched 2 completed jobs
    Updating Completed Jobs and creating new ones
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_12.Hmmpfam.1045801665.497.out /tmp/protein_analysis_pipe_.job_12.Hmmpfam.1045801665.497.err
    Creating 1 jobs
    Running job /tmp/protein_analysis_pipe_.job_13.Hmmpfam.1045801665.427.out /tmp/protein_analysis_pipe_.job_13.Hmmpfam.1045801665.427.err
    Going to snooze for 3 seconds...
    Waking up and run again!
    Fetching Jobs...
    Fetched 0 incomplete jobs
    Fetched 2 completed jobs
    Updating Completed Jobs and creating new ones
    Going to snooze for 3 seconds...
    Waking up and run again!
    Nothing left to run.

    ///////////////Shutting Down Pipeline//////////////////////
    Removing Lock File...
    Done
    ///////////////////////////////////////////////////////////
    savikalpa@pulse1 ~/src/bioperl-pipeline/scripts>                                                                             
   </programlisting>
  <para>
   Once you are ready to run on the node, submit the jobs to the compute nodes now by running the PipelineManger without the -l option.
   It is recommended for those who is running this on a desktop setup to use the nice command to set a lower priority like 15 so that
   your computer doesn't hang up on a large process.
  </para>
  <programlisting>
   savikalpa@pulse2 ~/src/bioperl-pipeline/scripts> perl PipelineManager -dbname blast_pipe -xml ../xml/templates/blast_file_pipeline.xml -queue   priority -batchsize 1
   A database called blast_pipe already exists.
   Continuing would involve dropping this database and loading a fresh one using ../xml/templates/blast_file_pipeline.xml.
   Would you like to continue? y/n [n] y
   Dropping Databases
   Creating blast_pipe
   Loading Schema...
   Reading Data_setup xml   : ../xml/templates/blast_file_pipeline.xml
   Doing DBAdaptor and IOHandler setup
   Doing Pipeline Flow Setup
   Doing Analysis..
   Doing Rules
   Doing Job Setup...
   Loading of pipeline blast_pipe completed
   2 analysis found.
   Running test and setup..

   //////////// Analysis Test ////////////
   Checking Analysis 1 DataMonger
   -------------------- WARNING ---------------------
   MSG: Skipping test for DataMonger
   ---------------------------------------------------
    ok
   Checking Analysis 2 Blast ok
   Fetching Jobs...
   Fetched 1 incomplete jobs
   opening bsub command line:
    bsub -o /tmp/blast_pipe_DataMonger.1046052341.189.out -e /tmp/blast_pipe_DataMonger.1046052341.189.err -q priority -C0  /usr/users/savikalpa   /src/bioperl-pipeline//scripts/runner.pl -dbname blast_pipe -host mysql -port 3306 -dbuser root 1
   Fetched 0 completed jobs
   Going to snooze for 3 seconds...
   Waking up and run again!
   Fetching Jobs...
   Fetched 0 incomplete jobs
   Fetched 0 completed jobs
   Going to snooze for 3 seconds...
   Waking up and run again!
   Fetching Jobs...
   Fetched 0 incomplete jobs
   Fetched 0 completed jobs
   Going to snooze for 3 seconds...
   Waking up and run again!
 

</programlisting>

<para> At any one time, you may check in the job table for the status of your jobs. The following shows
an example mysql session</para>
 <programlisting>must be changed
   savikalpa@pulse2 ~> mysql -u root blast_pipe
   Reading table information for completion of table and column names
   You can turn off this feature to get a quicker startup with -A

   Welcome to the MySQL monitor.  Commands end with ; or \g.
   Your MySQL connection id is 4757040 to server version: 3.23.36

   Type 'help;' or '\h' for help. Type '\c' to clear the buffer

   mysql> select * from job;
--------+------------+-------------+----------+------------------------------------------+------------------------------------------+------------------------------------------+-----------+----------+---------------------+-------------+
| job_id | process_id | analysis_id | queue_id | stdout_file                              | stderr_file                              | object_file                              | status    | stage    | time                | retry_count |
+--------+------------+-------------+----------+------------------------------------------+------------------------------------------+------------------------------------------+-----------+----------+---------------------+-------------+
|      2 | NEW        |           2 |        0 | /tmp/blast_pipe_Blast.1046052808.573.out | /tmp/blast_pipe_Blast.1046052808.573.err | /tmp/blast_pipe_Blast.1046052808.573.obj | COMPLETED | UPDATING | 2003-02-24 10:13:35 |           0 |
|      3 | NEW        |           2 |        0 | /tmp/blast_pipe_Blast.1046052808.73.out  | /tmp/blast_pipe_Blast.1046052808.73.err  | /tmp/blast_pipe_Blast.1046052808.73.obj  | SUBMITTED | RUNNING  | 2003-02-24 10:13:36 |           0 |
|      4 | NEW        |           2 |        0 | /tmp/blast_pipe_Blast.1046052808.170.out | /tmp/blast_pipe_Blast.1046052808.170.err | /tmp/blast_pipe_Blast.1046052808.170.obj | NEW       |          | 2003-02-24 10:13:36 |           0 |
|      5 | NEW        |           2 |        0 | /tmp/blast_pipe_Blast.1046052809.698.out | /tmp/blast_pipe_Blast.1046052809.698.err | /tmp/blast_pipe_Blast.1046052809.698.obj | NEW       |          | 2003-02-24 10:13:36 |           0 |
|      6 | NEW        |           2 |        0 | /tmp/blast_pipe_Blast.1046052809.264.out | /tmp/blast_pipe_Blast.1046052809.264.err | /tmp/blast_pipe_Blast.1046052809.264.obj | NEW       |          | 2003-02-24 10:13:36 |           0 |
+--------+------------+-------------+----------+------------------------------------------+------------------------------------------+------------------------------------------+-----------+----------+---------------------+-------------+
5 rows in set (0.05 sec)

mysql> 
 
</programlisting>

  <para>
  Here we can see that one blast job has been created and is currently having status <emphasis>SUBMITTED</emphasis> and stage
  <emphasis>RUNNING</emphasis>. The status available are NEW|SUBMITTED|FAILED|COMPLETED and for stage are BATCHED|READING|RUNNING|WRITING.
  Once a job is completed, the jobs will be move to the completed_jobs table.
  If the job fails, you may view the error log specified by stderr_file.
  Currently stderr_file and stdout_files are removed only if a job is completed. 
  </para>

</section>
  <section id="ViewingYourResults">
  <title>Viewing Your Results</title>
  <para>You may now check the <emphasis>resultdir</emphasis> specified in the Blast Analysis to 
  see if your blast results are stored properly.
  </para>

  <para>Congratulations at this point you have managed to setup the biopipe successfully. It is
  hoped that you have a feel of how the biopipe works. You may now try out more complex examples of 
  through the other XML templates that we have. More documentation for this will come in the future.
  </para>
  </section>
    
    
</article>
