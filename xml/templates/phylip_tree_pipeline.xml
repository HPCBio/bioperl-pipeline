<!--

phylip_tree_pipeline.xml

A Phylogenetic Tree Building Pipeline that uses the Phylip Suite 

19 Nov 2002
Cared for by Shawn Hoon <shawnh@fugu-sg.org>
http://www.biopipe.org

<PIPELINE SUMMARY>

Given files containing family of proteins run it through the following pipeline:

Proteins->Clustalw or TCoffee ->SeqBoot->ProtDist->Neighbor->Consense->DrawTree

This is a file based analysis, that spits out files at each analysis.

</PIPELINE SUMMARY>

<DESCRIPTION>

This pipeline is a phylogenetic pipeline that wraps certain phylip
programs and allow for the jobs to be distributed.  It is currently
tested and working on Phylip 3.5 but should work with Phylip3.6
with some configuration. The aim is to allow objects to written
both to database or files depending on one's specification.

</DESCRIPTION>

<THE PHYLO PIPELINE>

  <INSTALLATION REQUIREMENTS>

    <PERL PACKAGES>
      Standard Biopipe installation (pls refer to bioperl-pipeline/INSTALL)
    </PERL PACKAGES>

    <BINARIES>
      1. PHYLIP, a free package of programs for inferring phylogenies
         see: http://evolution.genetics.washington.edu/phylip.html
      2. clustalw: To align a group of Protein Sequences in table format.
         see: http://bmerc-www.bu.edu/examples/clustalw.html
      3. Alternative to clustalw, you may install TCoffee, another multiple
         sequence alignment program
         see: http://igs-server.cnrs-mrs.fr/~cnotred/Projects_home_page/t_coffee_home_page.html
    </BINARIES>
 
  <CONFIGURING PIPELINE>
    <global 
          rootdir="t/"
          datadir="$rootdir/data"
          workdir="$datadir/phylipdir"
          resultdir="$datadir/phylip_result"
          inputfile="$datadir/cysprot.fa"
          msa_logicname="Clustalw"
          msa_program= "clustalw"
          msa_param = "-ktuple 2 -matrix BLOSUM"
    />

    rootdir - specifies base directory where subsequent directories point to
    datadir - directory where data files are stored 
    workdir - working directory where input sequences are split. Each job will consist
              of a single input file
    resultdir - directory where the results of the phylo programs are stored. Each
                file will have a different suffix based on the program:
                cls - MSA(Multiple Sequence Alignment program
                sb  - SeqBoot
                pd  - ProtDist
                nb  - Neighbor
                con - consense
                ps  - Draw Tree (postscript file)
    inputfile - the path to the input file containing all the input protein sequences 
    msa_logicname - the logic name of the MSA program. Either Clustalw of TCoffee (Case important)
    msa_programname = the actual binary name. Either "clustalw or t_coffee" 
    msa_param   - the parameters passed to the MSA programs

    In theory one can also substitute different tree building program but currently
    its not configurable using global variables. But it is trivial to modfiy the
    XML to use for example Molphy's ProtML. This invovles removing the ProtDist and Neighbor
    analysis with the ProtML analysis.

  </CONFIGURING PIPELINE>    
  <LOADING PIPELINE>
      
       The pipeline is loaded up using this XML file.
       A new database will be automtically created maybe created for you. 
       This is done using the PipelineManager script found in bioperl-pipeline/scripts.
       Using the script:

      ************************************
      *PipelineManager
      ************************************
      This is the central script used to run the pipeline.

      Usage: PipelineManager -dbname test_pipe -xml template/blast_file_pipeline.xml -local 

      Options:
      Default values are read from PipeConf.pm

           -dbhost    The database host name (localhost)
           -dbname    The pipeline database name (annotate_pipeline)
           -dbuser    User for connecting to db (root)
           -dbpass    The password to mysql database()
           -dbdriver  Database driver (mysql)
           -schema    The Biopipe database schema (../sql/schema)
           -xml       The xml pipeline template file. It will run XMLImporter if provided
           -xf        Force drop of any existing Biopipe database with the same name
           -flush     flush all locks on pipeline and remove any that exists. 
                      Should only be used for debugging or development.
           -batchsize The number ofjobs to be batched to one node
           -local     Whether to run jobs in local mode 
                      (on the node where this script is run)
           -jobnbr    Number of jobs to run (for testing)
           -queue     Specify the queue on which to submit jobs
           -retry     Number of times to retry failed jobs
           -notest    Don't run pre-pipeline checks
           -norun     Use when you just want to load the XML without running
           -verbose   Whether to show warning during test and setup
           -help      Display this help

        Note that -dbhost, -dbname, -dbuser, -dbpass can also be specified in 
        the Bio/Pipeline/PipeConf.pm file, for convenience.

  </LOADING THE PIPELINE>

  <RUNNING THE PIPELINE>

    Go to bioperl-pipeline/Bio/Pipeline

    Edit PipeConf.pm accordingly for your environment variables.

    Go to bioperl-pipeline/scripts, to load the pipeline without running:

    perl PipelineManager -dbname mydbname \
                         -dbuser <user> \
                         -dbhost <host> \
                         -xml ~/biopipe_release/xml/templates/examble/blast_file_pipeline.xml
                         -norun 

    run the pipeline:

    
    perl PipelineManager -dbname mydbname \
                         -dbuser <user> \
                         -dbhost <host> \
                         -queue priority

    you may use the -local option to run it in local mode without submitting to the nodes yet.
    This will be a good test before submitting, and is recommended for debugging.
  
    If you encounter any errors, you can simply rerun the first command with the xml option
    and the biopipe database will be automatically recreated once the errors have been fixed.

  </RUNNING THE PIPELINE>

  <AFTER NOTE>

      Running pipelines are inherently for the brave hearted and we are glad you are willing to give
      this a shot. We are working hard to ensure that it works as smoothly as possible.
      Do let us know any problems that you face and suggestions that you have and we will do us best to help. 

      In return, we ask that you keep a note down of your installation process and feedback that to us
      so that we may make changes or improve our documentation.

      cheers,

      The Fugu Team

   </AFTER NOTE>

<CHANGE LOG>
19 Dec 2002 - First Addition - Shawn
03 Jan 2003 - Docs slightly Modified - Juguang
11 Apr 2003 - Reorganized using global variables
</CHANGE LOG>

//-->

<pipeline_setup>
    <global 
          rootdir="t/"
          datadir="$rootdir/data"
          resultdir="$datadir/phylip_result"
          workdir="$datadir/phylipdir"
          inputfile="$datadir/cysprot.fa"
          msa_logicname="Clustalw"
          msa_program= "clustalw"
          msa_param = "-ktuple 2 -matrix BLOSUM"
    />
  <database_setup>
    <streamadaptor id="1">
      <module>Bio::Pipeline::Utils::Dumper</module>
    </streamadaptor>
  </database_setup>

  <!-- fetch the sequence -->
  <iohandler_setup>
    <iohandler id="1">
    <adaptor_id>1</adaptor_id>
    <adaptor_type>STREAM</adaptor_type>
    <iohandler_type>OUTPUT</iohandler_type>
    <method>
      <name>new</name>
      <argument>
        <tag>-dir</tag>
        <value>$resultdir</value>
      </argument>
      <argument>
        <tag>-file_suffix</tag>
        <value>cls</value>
      </argument>
      <argument>
        <tag>-prefix</tag>
        <value>INPUT</value>
        <type>SCALAR</type>
      </argument>
      <argument>
        <tag>-module</tag>
        <value>generic</value>
      </argument>
      <argument>
        <tag>-format</tag>
        <value>phylip</value>
      </argument>
    </method>
    <method>
     <name>dump</name>
     <argument>
       <value>OUTPUT</value>
     </argument>
     </method>
     </iohandler>
   <iohandler id="2">
    <adaptor_id>1</adaptor_id>
    <adaptor_type>STREAM</adaptor_type>
    <iohandler_type>OUTPUT</iohandler_type>
    <method>
      <name>new</name>
      <argument>
        <tag>-dir</tag>
        <value>	$resultdir</value>
      </argument>
      <argument>
        <tag>-file_suffix</tag>
        <value>sb</value>
      </argument>
      <argument>
        <tag>-prefix</tag>
        <value>INPUT</value>
        <type>SCALAR</type>
      </argument>
      <argument>
        <tag>-module</tag>
        <value>generic</value>
      </argument>
      <argument>
        <tag>-format</tag>
        <value>phylip</value>
      </argument>
    </method>
    <method>
     <name>dump</name>
     <argument>
       <value>OUTPUT</value>
     </argument>
     </method>
     </iohandler>
   <iohandler id="3">
    <adaptor_id>1</adaptor_id>
    <adaptor_type>STREAM</adaptor_type>
    <iohandler_type>OUTPUT</iohandler_type>
    <method>
      <name>new</name>
      <argument>
        <tag>-dir</tag>
        <value>	$resultdir</value>
      </argument>
      <argument>
        <tag>-file_suffix</tag>
        <value>pd</value>
      </argument>
      <argument>
        <tag>-prefix</tag>
        <value>INPUT</value>
        <type>SCALAR</type>
      </argument>
      <argument>
        <tag>-module</tag>
        <value>generic</value>
      </argument>
      <argument>
        <tag>-format</tag>
        <value>phylip</value>
      </argument>
    </method>
    <method>
     <name>dump</name>
     <argument>
       <value>OUTPUT</value>
     </argument>
     </method>
     </iohandler>
   <iohandler id="4">
    <adaptor_id>1</adaptor_id>
    <adaptor_type>STREAM</adaptor_type>
    <iohandler_type>OUTPUT</iohandler_type>
    <method>
      <name>new</name>
      <argument>
        <tag>-dir</tag>
        <value>	$resultdir</value>
      </argument>
      <argument>
        <tag>-file_suffix</tag>
        <value>nb</value>
      </argument>
      <argument>
        <tag>-prefix</tag>
        <value>INPUT</value>
        <type>SCALAR</type>
      </argument>
      <argument>
        <tag>-module</tag>
        <value>generic</value>
      </argument>
      <argument>
        <tag>-format</tag>
        <value>newick</value>
      </argument>
    </method>
    <method>
     <name>dump</name>
     <argument>
       <value>OUTPUT</value>
     </argument>
     </method>
     </iohandler>
   <iohandler id="5">
    <adaptor_id>1</adaptor_id>
    <adaptor_type>STREAM</adaptor_type>
    <iohandler_type>OUTPUT</iohandler_type>
    <method>
      <name>new</name>
      <argument>
        <tag>-dir</tag>
        <value>	$resultdir</value>
      </argument>
      <argument>
        <tag>-file_suffix</tag>
        <value>con</value>
      </argument>
      <argument>
        <tag>-prefix</tag>
        <value>INPUT</value>
        <type>SCALAR</type>
      </argument>
      <argument>
        <tag>-module</tag>
        <value>generic</value>
      </argument>
      <argument>
        <tag>-format</tag>
        <value>newick</value>
      </argument>
    </method>
    <method>
     <name>dump</name>
     <argument>
       <value>OUTPUT</value>
     </argument>
     </method>
     </iohandler>
  </iohandler_setup>

        
  <pipeline_flow_setup>
    <analysis id="1">
      <data_monger>
        <initial/>
        <input>
          <name>input_file</name>
        </input>
        <input_create>
           <module>setup_file</module>
           <rank>1</rank>
           <argument>
             <tag>tag</tag>
             <value>infile</value>
           </argument>
            <argument>
                <tag>input_file</tag>
                <value>$inputfile</value>
                <type>SCALAR</type>
            </argument>
            <argument>
                <tag>chop_nbr</tag>
                <value>1</value>
                <type>SCALAR</type>
            </argument>
            <argument>
                <tag>workdir</tag>
                <value>$workdir</value>
                <type>SCALAR</type>
            </argument>
            <argument>
                <tag>result_dir</tag>
                <value>	$resultdir</value>
                <type>SCALAR</type>
            </argument>
            <argument>
                <tag>runnable</tag>
                <value>Bio::Pipeline::Runnable::MSA</value>
                <type>SCALAR</type>
            </argument>
            <argument>
                <tag>informat</tag>
                <value>fasta</value>
                <type>SCALAR</type>
            </argument>
            <argument>
                <tag>outformat</tag>
                <value>fasta</value>
                <type>SCALAR</type>
            </argument>
         </input_create>
      </data_monger>
    </analysis>


    <analysis id="2">
      <logic_name>$msa_logicname</logic_name>
      <runnable>Bio::Pipeline::Runnable::MSA</runnable>
      <program>$msa_program</program>
      <analysis_parameters>$msa_param</analysis_parameters>
      <runnable_parameters>-program $msa_logicname -align_type align -infile_dir $workdir </runnable_parameters>
      <output_iohandler id="1"/>
    </analysis>

    <analysis id="3">
      <logic_name>Phylip</logic_name>
      <runnable>Bio::Pipeline::Runnable::Phylip</runnable>
      <runnable_parameters>-program SeqBoot -infile_suffix cls -infile_dir 	$resultdir</runnable_parameters>
      <analysis_parameters>-replicates 10 </analysis_parameters>
      <output_iohandler id="2"/>
    </analysis>
    <analysis id="4">
      <logic_name>Phylip</logic_name>
      <runnable>Bio::Pipeline::Runnable::Phylip</runnable>
      <runnable_parameters>-program ProtDist -infile_suffix sb -infile_dir 	$resultdir</runnable_parameters>
      <analysis_parameters>-multiple 10 </analysis_parameters>
      <output_iohandler id="3"/>
    </analysis>
    <analysis id="5">
      <logic_name>Phylip</logic_name>
      <runnable>Bio::Pipeline::Runnable::Phylip</runnable>
      <runnable_parameters>-program Neighbor -infile_suffix pd -infile_dir 	$resultdir</runnable_parameters>
      <analysis_parameters>-multiple 10 </analysis_parameters>
      <output_iohandler id="4"/>
    </analysis>
    <analysis id="6">
      <logic_name>Phylip</logic_name>
      <runnable>Bio::Pipeline::Runnable::Phylip</runnable>
      <runnable_parameters>-program Consense -infile_suffix nb -infile_dir 	$resultdir</runnable_parameters>
      <output_iohandler id="5"/>
    </analysis>
    <analysis id="7">
      <logic_name>Phylip</logic_name>
      <runnable>Bio::Pipeline::Runnable::Phylip</runnable>
      <runnable_parameters>-program DrawTree -infile_suffix con -infile_dir 	$resultdir</runnable_parameters>
    </analysis>
    <rule>
      <current_analysis_id>1</current_analysis_id>
     <next_analysis_id>2</next_analysis_id>
      <action>NOTHING</action>
    </rule>
    <rule>
      <current_analysis_id>2</current_analysis_id>
     <next_analysis_id>3</next_analysis_id>
      <action>COPY_ID_FILE</action>
    </rule>
    <rule>
      <current_analysis_id>3</current_analysis_id>
     <next_analysis_id>4</next_analysis_id>
      <action>COPY_ID_FILE</action>
    </rule>
    <rule>
      <current_analysis_id>4</current_analysis_id>
     <next_analysis_id>5</next_analysis_id>
      <action>COPY_ID_FILE</action>
    </rule>
    <rule>
      <current_analysis_id>5</current_analysis_id>
     <next_analysis_id>6</next_analysis_id>
      <action>COPY_ID_FILE</action>
    </rule>
    <rule>
      <current_analysis_id>6</current_analysis_id>
     <next_analysis_id>7</next_analysis_id>
      <action>COPY_ID_FILE</action>
    </rule>

  </pipeline_flow_setup>

  <job_setup>
 </job_setup>

</pipeline_setup>
